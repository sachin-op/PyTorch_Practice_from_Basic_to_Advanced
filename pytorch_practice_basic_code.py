# -*- coding: utf-8 -*-
"""Pytorch_Practice_Basic_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RXieRMgLJ2dztnJnvjREH737HEuSVDov

# **What is pyTorch?**


*   Most popular research deep learning Framework.
*   Write fast deep learning code in python(able to run on gpu/many GPUs)
*   Able to acsess many pre-built deep learning models(Torch Hub/torchvision.models)
*   Whole process:Preprocess data,model data,deploy model in your application/cloud
*   Originally designed and used in-house by Facebook/Meta(now open-source and used by companies such as Tesla,microsoft,OpenAi)
"""

#Importing PyTorch
import torch

#importin numpy
import numpy as np

#Checking PyTorch Version
print(torch.__version__)

"""## **Introduction to Tensor**

**Creating Tensor**

PyTorch tensor are created using `torch.Tensor()`= [read through the PyTorch documentation Page](https://docs.pytorch.org/docs/stable/tensors.html)

### **Scaler**

A scalar is a single number and in tensor-speak it's a zero dimension tensor.
"""

#scalar
scalar = torch.tensor(7)
scalar

# dimension of scaler
scalar.ndim

# Get the Python number within a tensor (only works with one-element tensors)
scalar.item()

"""### **Vector**

A vector is a single dimension tensor but can contain many numbers.
"""

#vector
vector = torch.tensor([7,7])

print(vector)

# Check the number of dimensions of vector
print(vector.ndim)

# Check shape of vector
print(vector.shape)

"""### **Matrix**

A matrix is a 2-dimensions array of numbers: rows × columns.

"""

#matrix
matrix = torch.tensor([[7,8],[9,10]])

# Print the entire matrix
print(matrix)

# Check and print the number of dimensions (e.g., 2 for a 2D matrix)
print(matrix.ndim)

# Check and print the shape of the matrix (rows, columns)
print(matrix.shape)

## Access and print the second row of the matrix (index 1)
print(matrix[1])

"""### **Tensor**

An n-dimensional array of numbers
"""

# Create a 3D tensor with shape (1, 3, 3)
tensor = torch.tensor([[[1, 2, 3],
                        [3, 6, 9],
                        [2, 4, 5]]])

# Print the entire tensor
print(tensor)

# Print the number of dimensions (3D tensor → ndim = 3)
print(tensor.ndim)

# Print the shape of the tensor (1 block, 3 rows, 3 columns)
print(tensor.shape)

# Access and print the first (and only) 2D matrix block
print(tensor[0])

# Access and print the second row (index 1) of the first 2D block
print(tensor[0][1])

# Access and print the first element (index 0) of the second row
print(tensor[0][1][0])

"""### Random Tensor

**Why random tensor?**

Random tensor are important the way many neural networks learn is that they start with tensors full of random numbers and then adjust those random numbers to better represent the data.

PyTorch random tensor are using `torch.rand()`:- [PyTorch documentation for random page](https://docs.pytorch.org/docs/stable/generated/torch.rand.html)
"""

#Create a random tensor of shape (3, 4) → 2D tensor with 3 rows and 4 columns
random_tensor = torch.rand(3,4)
random_tensor

#Access and print the first row of the tensor (index 0)
random_tensor[0]

#Print the number of dimensions (should be 2 for a 2D tensor)
random_tensor.ndim

#Create a random tensor of shape (1, 10, 10) → 3D tensor (like a grayscale image batch)
random_tensor1 = torch.rand(1,10,10)
random_tensor1

#Print the number of dimensions
random_tensor1.ndim

#Print the shape and number of dimensions of the image-like tensor
random_image_size_tensor = torch.rand(size=(224,224,3)) # Should be torch.Size([224, 224, 3])
random_image_size_tensor.shape,random_image_size_tensor.ndim # Should be 3 (height, width, channels)

"""### Zeros and Ones

Create a tensor full of zeros with `torch.zeros()`
"""

# Create a 2x3 tensor filled with zeros
zeros = torch.zeros(2, 3)
print("2x3 Zero Tensor:\n", zeros)

# Create a 3x4 tensor filled with zeros using 'size=' syntax
zeros1 = torch.zeros(size=(3, 4))
print("3x4 Zero Tensor:\n", zeros1)

"""Create a tensor of all ones using `torch.ones()`"""

# Create a 3x4 tensor filled with ones
ones = torch.ones(3, 4)
print("3x4 Ones Tensor (using direct shape):\n", ones)

# Create another 3x4 tensor of ones using 'size=' syntax
ones1 = torch.ones(size=(3, 4))
print("3x4 Ones Tensor (using size=):\n", ones1)

# Check data type of the ones tensor
print("Data type of 'ones' tensor:", ones.dtype)

# Create a random tensor to compare dtype
random_tensor = torch.rand(3, 4)
print("Data type of 'random_tensor':", random_tensor.dtype)

"""### Tensor Range

Creating range of tensors and tensors-like
"""

# use torch.range()
torch.range(0,10,1)
## torch.range is deprecated!

# Create a tensor with values from 0 to 9 with step size 1
zero_to_ten = torch.arange(0, 10, 1)
print("Tensor from 0 to 9:\n", zero_to_ten)

# Create a tensor of zeros having the same shape as `zero_to_ten`
ten_zeros = torch.zeros_like(zero_to_ten)
print("Zeros like 'zero_to_ten':\n", ten_zeros)

"""### Tensor datatypes

There are many different [tensor datatypes available in PyTorch.](https://docs.pytorch.org/docs/stable/tensors.html#data-types)

A `torch.Tensor` is a multi-dimensional matrix containing elements of a single data type.


**Notes:** Tensor datatype is one of the 3 big errors you'll run into the pytorch & deep learning:
1. Tensors not right datatype
2. Tensors not right shape
3. Tensors not on the right device

* The most common type (and generally the default) is `torch.float32` or `torch.float`.

* But there's also 16-bit floating point (`torch.float16` or `torch.half`) and 64-bit floating point (`torch.float64` or `torch.double`).
"""

# Create a float32 tensor manually
float_32_tensor = torch.tensor([3.0, 6.0, 9.0],
                               dtype=None, # defaults to None, which is torch.float32 or whatever datatype is passed
                               device=None, # defaults to None, which uses the default tensor type
                               requires_grad=False,) # if True, operations performed on the tensor are recorded
print("Float32 Tensor:-\n", float_32_tensor)
print("Data type:", float_32_tensor.dtype)  # should be torch.float32
print("device:",float_32_tensor.device)

# Explicitly create a float32 tensor
float_32_tensor1 = torch.tensor([3.0, 6.0, 9.0], dtype=torch.float32)
print("Explicit Float32 Tensor:\n", float_32_tensor1)
print("Data type:", float_32_tensor1.dtype)

# Convert float32 tensor to float16
float_16_tensor = float_32_tensor1.type(torch.float16)
print("Float16 Tensor:\n", float_16_tensor)

# Multiply float16 tensor with float32 tensor (auto type promotion will happen)
product = float_16_tensor * float_32_tensor1
print("Product (float16 * float32):\n", product)

# Create an int32 tensor
int_32_tensor = torch.tensor([3, 6, 9], dtype=torch.int32)
print("Int32 Tensor:\n", int_32_tensor)

# Multiply float32 tensor with int32 tensor (int will be promoted to float automatically)
mixed_product = float_32_tensor * int_32_tensor
print("Product (float32 * int32):\n", mixed_product)

int_32_tensor = torch.tensor([3, 6, 9], dtype=torch.int32)
int_32_tensor

float_32_tensor * int_32_tensor

"""### Getting information from tensors

1. Tensor not right datatype-to do datatype from a tensor,can use `tensor.dtype`
2. Tensor not right shape-to get shape from a tensor,can use tensor.shape
3. Tensors not on the right device-to get device from a tensor,can use tensor.device
"""

# Create a tensor
some_tensor = torch.rand(3,4)
some_tensor

# Find out details about some tensor
print(some_tensor)
print(f"Datatype of tensor:{some_tensor.dtype}")
print(f"Shape of tensor:{some_tensor.shape}")
print(f"Device tensor is on:{some_tensor.device}")

"""### Manipulating Tensors(Tensor Operations)

Tensor operations include:
* Addition `torch.add()` method(function)
* Subtraction `torch.sub()` method(function)
* Multiplication(element-wise) `torchmul()` method(function)
* Division `torchdiv()` method(function)
* Matrix multiplication `torchmatmul()` method(function)
"""

# Create a 1D tensor
tensor = torch.tensor([1, 2, 3])
print("Original Tensor:\n", tensor)

# 1. Addition
add_tensor = tensor + 10                    # Using operator
add_tensor_func = torch.add(tensor, 10)     # Using functional API
print("\nTensor + 10 (operator):\n", add_tensor)
print("Tensor + 10 (torch.add):\n", add_tensor_func)

# 2. Subtraction
sub_tensor = tensor - 10
sub_tensor_func = torch.sub(tensor, 10)
print("\nTensor - 10 (operator):\n", sub_tensor)
print("Tensor - 10 (torch.sub):\n", sub_tensor_func)

# 3. Multiplication (Element-wise)
mul_tensor = tensor * 10
mul_tensor_func = torch.mul(tensor, 10)
print("\nTensor * 10 (operator):\n", mul_tensor)
print("Tensor * 10 (torch.mul):\n", mul_tensor_func)

# 4. Division (Element-wise)
div_tensor = tensor / 2
div_tensor_func = torch.div(tensor, 2)
print("\nTensor / 2 (operator):\n", div_tensor)
print("Tensor / 2 (torch.div):\n", div_tensor_func)

# In-place operations (modifies original tensor)
tensor_clone = tensor.clone()  # Always clone before in-place ops to avoid side effects
tensor_clone.add_(5)           # Adds 5 to each element (in-place)
print("\nIn-place addition (add_):\n", tensor_clone)

# Matrix Multiplication
# Define 2 matrices for multiplication
A = torch.tensor([[1, 2], [3, 4]])
B = torch.tensor([[5, 6], [7, 8]])
# Matrix multiplication using @ operator and torch.matmul
matmul_result = A @ B
matmul_func = torch.matmul(A, B)

# Broadcasting smaller tensor to match shape
tensor_2d = torch.tensor([[1, 2, 3], [4, 5, 6]])
broadcast_tensor = tensor_2d + tensor      # tensor is [1, 2, 3]
print("\nBroadcasting Example (2D + 1D):\n", broadcast_tensor)

"""### Matrix multiplication

Two main ways of performaing multiplication in neural networks and deep learning:
1. Elements-wise multiplication
2. Matrix multiplication (dot product)

multiplying matrices-https://www.mathsisfun.com/algebra/matrix-multiplying.html

There are two main rules that performing matrix multiplication needs to satisfy:
1. The **inner dimension** must match:
* `(3,2) @ (3,2)` won't work
* `(2,3) @ (3,2)` will work
* `(3,2) @ (2,3)` will work
2. The resulting matrix has the shape of the **Outer dimensions:**

* (2,3) @ (3,2) -> (2,2)
* (3,2) @ (2,3) -> (3,3)

1. Element-wise Multiplication
"""

# Define a simple tensor
tensor = torch.tensor([1, 2, 3])
print("Original Tensor:\n", tensor)

# Element-wise multiplication
elementwise = tensor * tensor
print("\nElement-wise Multiplication:\n", tensor, "*", tensor)
print("Result:", elementwise)

# Equivalent using torch.mul()
elementwise_func = torch.mul(tensor, tensor)
print("Using torch.mul():", elementwise_func)

"""2. Matrix Multiplication (Dot Product)

"""

# For 1D tensor dot product, matmul acts like a dot product
dot_product = torch.matmul(tensor, tensor)  # 1x3 @ 3x1 => scalar
print("\nMatrix Multiplication (Dot Product) using torch.matmul:\n", dot_product)

# Manual calculation of dot product: 1*1 + 2*2 + 3*3 = 14
manual_dot = sum([tensor[i] * tensor[i] for i in range(len(tensor))])
print("Manual Dot Product Calculation:\n", manual_dot)

"""3. Matrix Multiplication (2D)

"""

# Create two 2D tensors with compatible shapes
f = torch.randint(size=(2, 3), low=0, high=10)
g = torch.randint(size=(3, 2), low=0, high=10)
print("\nMatrix f (2x3):\n", f)
print("Matrix g (3x2):\n", g)

# Perform matrix multiplication
fg_result = torch.matmul(f, g)  # Shape (2,3) @ (3,2) -> (2,2)
print("\nMatrix Multiplication f @ g => Shape (2,2):\n", fg_result)

"""4. Shape Rule Recap (IMPORTANT!)"""

# Inner dimensions must match:
# (A x B) @ (B x C) = (A x C)

A = torch.rand(2, 3)
B = torch.rand(3, 2)
print("\nA shape:", A.shape)
print("B shape:", B.shape)
print("Resulting shape A @ B:", (A @ B).shape)  # Should be (2, 2)

C = torch.rand(3, 2)
D = torch.rand(2, 3)
print("\nC shape:", C.shape)
print("D shape:", D.shape)
print("Resulting shape C @ D:", (C @ D).shape)  # Should be (3, 3)

"""5. Timing: Manual vs torch.matmul"""

# Commented out IPython magic to ensure Python compatibility.
# # Manual dot product (inefficient for large tensors)
# %%time
# value = 0
# for i in range(len(tensor)):
#     value += tensor[i] * tensor[i]
# print("\nManual dot product value:", value)

# Commented out IPython magic to ensure Python compatibility.
# # torch.matmul dot product (efficient & optimized)
# %%time
# fast_result = torch.matmul(tensor, tensor)
# print("torch.matmul result:", fast_result)

"""### One of the most common errors in deep learning:-shape errors
* Fixing Shape Errors with Transpose in Matrix Multiplication.

1. Define two tensors
"""

# Tensor A has shape (3, 2)
tensor_A = torch.tensor([
    [1, 2],
    [3, 4],
    [5, 6]
])
# Tensor B has shape (3, 2) — same number of rows as A, but we want to multiply them
tensor_B = torch.tensor([
    [7, 10],
    [8, 11],
    [9, 12]
])

print("Tensor A:\n", tensor_A)
print("Shape of A:", tensor_A.shape)

print("\nTensor B:\n", tensor_B)
print("Shape of B:", tensor_B.shape)

""" 2. Shape Error Example

"""

# Matrix multiplication (3x2) @ (3x2) — invalid! inner dimensions don't match (2 ≠ 3)

# This will throw an error if uncommented:
# result = torch.matmul(tensor_A, tensor_B)

"""3. Fix using Transpose"""

# Transpose B: Changes shape from (3, 2) -> (2, 3)
tensor_B_T = tensor_B.T
print("\nTransposed Tensor B:\n", tensor_B_T)
print("Shape of Transposed B:", tensor_B_T.shape)

# Now we can do matrix multiplication: (3x2) @ (2x3) -> (3x3)
output = torch.matmul(tensor_A, tensor_B_T)

print("\n------------------------------")
print(f"Original shapes: tensor_A = {tensor_A.shape}, tensor_B = {tensor_B.shape}")
print(f"New shapes for multiplication: tensor_A = {tensor_A.shape}, tensor_B.T = {tensor_B_T.shape}")
print(f"Multiplying: {tensor_A.shape} @ {tensor_B_T.shape} -> Result shape = {output.shape}")
print("------------------------------")

"""4. Final Output"""

print("Matrix Multiplication Result:\n", output)
print("Output shape:", output.shape)

"""Tip"""

# tensor.T  # For quick 2D transpose - Note: This will give a UserWarning for tensors with more than 2 dimensions. Use .mT or .permute instead for higher dimensions.

# permute() requires a 'dims' argument to specify the new order of dimensions.
# Example with a 3D tensor:
# Create a 3D tensor with shape (2, 3, 4)
x_3d = torch.rand(2, 3, 4)
print("Original 3D tensor shape:", x_3d.shape)

# Permute the dimensions to (4, 2, 3)
x_permuted = x_3d.permute(2, 0, 1)
print("Permuted 3D tensor shape:", x_permuted.shape)

"""To fix our tensor shape issues, we can manipulate the shape of one of our tensors using a **trnaspose.**

A **transpose** switches the axes or dimensions of a given tensor.

### Aggregation Operations
* Aggregation operations and Positional min/max in PyTorch using `.min()`, `.max()`, `.mean()`, `.sum()`, `torch.argmin()`, and `torch.argmax()`.

* Note: `torch.mean()` needs float dtype like float32, not int.
"""

# Create a tensor of integers from 0 to 90 with a step of 10
x = torch.arange(0, 100, 10)
print("Tensor:\n", x)

# Aggregation functions
print(f"Minimum: {x.min()}")       # Smallest value
print(f"Maximum: {x.max()}")       # Largest value

# Mean only works on float type tensors
# print(f"Mean: {x.mean()}")      #  Will raise error on int type

mean_val = x.type(torch.float32).mean()
print(f"Mean (float32 converted): {mean_val}")  #Note: torch.mean() needs float dtype like float32, not int.

print(f"Sum: {x.sum()}")           # Sum of all elements

"""Alternate Aggregation with Functional API:"""

# Use torch methods for aggregation
min_val = torch.min(x)
max_val = torch.max(x)
mean_val = torch.mean(x.type(torch.float32))
sum_val = torch.sum(x)

print((max_val, min_val, mean_val, sum_val))

"""###  Positional Min/Max (argmin, argmax)"""

# Create another tensor
tensor = torch.arange(10, 100, 10)
print("Tensor:\n", tensor)

# Get positions (indexes) of max and min values
max_index = torch.argmax(tensor)
min_index = torch.argmin(tensor)

print(f"Index where max value occurs: {max_index}")  # Position of 90
print(f"Index where min value occurs: {min_index}")  # Position of 10

"""## Reshaping, stacking,squeezing and unsqueezing tensors
* Reshaping - reshapes an input tensor to a defined shape
* View - Returns a view of an input tensor of certain shape but keep the same memory as the original tensor
* stacking - combine multiple tensors on top of each other(vstack) or side by side(hstack)
* Squeeze - removes all`1` dimensions from a tensor
* Unsqueeze - add a `1` dimension to a target tensor
* Permute - Return a view of the input with dimensions permuted (swaped) in a certain way

Tensor Manipulation Functions in PyTorch:

* Reshape   → reshape()     → Changes shape, creates new tensor

* View      → view()        → Changes shape, shares memory

* Stacking  → stack()       → Combines multiple tensors

* Squeeze   → squeeze()     → Removes dimensions of size 1

* Unsqueeze → unsqueeze()   → Adds dimensions of size 1

* Permute   → permute()     → Rearranges dimensions (e.g., HWC ➝ CHW)
"""

# Create a basic tensor
x = torch.arange(1., 10.)  # Tensor: [1, 2, ..., 9]
print("Original tensor:")
print(x)
print("Original shape:", x.shape)

# Reshape - reshapes an input tensor to a defined shape
x_reshaped = x.reshape(1, 9)
print("\nReshaped tensor:")
print(x_reshaped)
print("Reshaped shape:", x_reshaped.shape)

# View - similar to reshape but shares memory with original tensor
z = x.view(1, 9)
print("\nViewed tensor:")
print(z)
print("Viewed shape:", z.shape)

# Modifying view also modifies the original tensor
z[:, 0] = 99
print("\nModified view (z[:, 0] = 99):")
print("z:", z)
print("x (also changed):", x)

# Stacking - combine multiple tensors
x_stacked_v = torch.stack([x, x, x], dim=0)  # Stack vertically (along rows)
x_stacked_h = torch.stack([x, x, x], dim=1)  # Stack horizontally (along columns)
print("\nStacked Vertically (dim=0):")
print(x_stacked_v)
print("\nStacked Horizontally (dim=1):")
print(x_stacked_h)

# Squeeze - removes all 1 dimensions
print("\nBefore squeeze:")
print("x_reshaped:", x_reshaped)
print("x_reshaped shape:", x_reshaped.shape)


x_squeezed = x_reshaped.squeeze()
print("\nAfter squeeze:")
print("x_squeezed:", x_squeezed)
print("x_squeezed shape:", x_squeezed.shape)

# Unsqueeze - adds a 1 dimension at the specified position
print("\nBefore unsqueeze:")
print("x_squeezed:", x_squeezed)
print("x_squeezed shape:", x_squeezed.shape)

x_unsqueezed = x_squeezed.unsqueeze(dim=0)  # Add dimension at 0
print("\nAfter unsqueeze (dim=0):")
print("x_unsqueezed:", x_unsqueezed)
print("x_unsqueezed shape:", x_unsqueezed.shape)

# Permute - Rearranges dimensions of tensor
# e.g., simulate image tensor: [height, width, channels]
x_original = torch.rand(size=(224, 224, 3))
print("\nOriginal image shape (HWC):", x_original.shape)

# Convert to [channels, height, width] format (used in PyTorch CNNs)
x_permuted = x_original.permute(2, 0, 1)
print("Permuted shape (CHW):", x_permuted.shape)

# Verify memory sharing: Change a value and see if it reflects
x_original[0, 0, 0] = 123456.0
print("\nAfter modifying x_original[0,0,0]:")
print("x_original[0,0,0]:", x_original[0,0,0])
print("x_permuted[0,0,0]:", x_permuted[0,0,0])  # Should reflect change if shared

"""### Indexing(selecting datat from tensors)
 indexing with Pytorch is similar to indexing with Numpy
"""

# create a tensor
x = torch.arange(1,10).reshape(1,3,3)
x,x.shape

# Lets index on our new tensor
x[0]

# lets index on the middle bracket (dim=1)
x[0][0]

# lets index on the most inner bracket(last dimension)
x[0][0][0]

# You can also use ":" to select "all" of a target dimension
x[:,0]

# Get all values of 0th and 1st dimensions but only index 1 of 2nd dimensions
x[:,:,1]

# Get all values of the 0 dimension but only the 1 index value of 1st and 2nd dimension
x[:,1,1]

# get index 0 of 0th and 1st dimension and all values of 2nd dimension
x[0,0,:]

# Index on x to return 9
x

#Index on x to return 3,6,9
print(x[:,:,2])

x



"""##PyTorch tensor & numPy

NumPy is a popular scientific Python numerical computing library.
And because of this, PyTorch has functionality to interact with it.

* Data in NumPy,want in PyTorch tensor -> `torch.from_numpy(ndarray)`
* PyTorch tensor -> NumPy -> `torch.Tensor.numpy()`

NumPy array ➝ PyTorch tensor:

    - Use torch.from_numpy(ndarray)
    - Shares memory with original NumPy array
    - Must manually set dtype if needed (e.g. float32)

PyTorch tensor ➝ NumPy array:

    - Use tensor.numpy()
    - Shares memory with original PyTorch tensor

Changes in one will reflect in the other unless cloned/copied
"""

#NumPy ↔ PyTorch Tensor Conversion
#NumPy array ➝ PyTorch Tensor
array = np.arange(1.0, 8.0)
print("Original NumPy array:", array)

# Convert NumPy array to PyTorch tensor (explicitly set dtype to float32)
tensor = torch.from_numpy(array).type(torch.float32)
print("Converted PyTorch tensor:", tensor)

# By default, from_numpy gives float64 if dtype not changed
print("NumPy dtype:", array.dtype)                       # float64
print("Default PyTorch dtype (from arange):", torch.arange(1.0, 8.0).dtype)

# Modify NumPy array ➝ tensor reflects change (shared memory)
array = array + 1
print("Modified NumPy array:", array)
print("Tensor after array change:", tensor)              # tensor also updated!

# PyTorch Tensor ➝ NumPy array
tensor = torch.ones(7)
numpy_tensor = tensor.numpy()
print("New PyTorch tensor:", tensor)
print("Converted NumPy tensor:", numpy_tensor)
print("NumPy tensor dtype:", numpy_tensor.dtype)

# Modify tensor ➝ numpy_tensor reflects change (shared memory)
tensor = tensor + 1
print("Modified Tensor:", tensor)
print("NumPy after tensor change:", numpy_tensor)        # numpy_tensor also updated!

"""## Reproducibility in PyTorch (Controlling Randomness))

In short how a neural network learners:

`start with random numbers -> tensor operation -> update random numbers to try and make them better representations of the data -> again ->again...

to reduce the randomness in neural networks and pytorch comes the concept of a **random seed**

Essentially what the random seed does is "flavour" the randomness.

"""

# Random tensors without seed (true randomness)
random_tensor_A = torch.rand(3, 4)
random_tensor_B = torch.rand(3, 4)

print("Random Tensor A:\n", random_tensor_A)
print("Random Tensor B:\n", random_tensor_B)
print("Are A and B equal?:", random_tensor_A == random_tensor_B)  # False

1 == 1

# Reproducible random tensors using manual seed
RANDOM_SEED = 42
torch.manual_seed(RANDOM_SEED)  # Lock randomness

random_tensor_C = torch.rand(3, 4)

torch.manual_seed(RANDOM_SEED)  # Reset seed again to get same values
random_tensor_D = torch.rand(3, 4)

print("\nRandom Tensor C:\n", random_tensor_C)
print("Random Tensor D (after re-seeding):\n", random_tensor_D)
print("Are C and D equal?:", random_tensor_C == random_tensor_D)  # True

# Without setting the seed → random output changes every run
torch.rand(2, 3)

# Set seed to make randomness reproducible
torch.manual_seed(42)

"""## Running tensors and Pytorch objects on the GPUs ( and making faster computations)
GPUs = faster computation on numbers,thanks to CUDA + NVIDIA hardware + PyTorch working behind the scenes to make everything good.

# 1. Running Tensors on GPUs in PyTorch

1.Easiest - Use Google colab for a free GPU (options to upgrade as well)
2.Use your own GPU - takes a little bit of setup and requires the investment of purchasing
3.Use cloud computing - GCP,AWS,Azure, these services allow you to rent computers on the cloud and access them

**Interoperability Between NumPy and PyTorch**

* PyTorch can convert arrays from and to NumPy efficiently.
"""

# NumPy array to PyTorch tensor
array = np.arange(1.0, 8.0)
tensor = torch.from_numpy(array).type(torch.float32)  # force float32 instead of default float64

print("NumPy array:", array)
print("Converted PyTorch tensor:", tensor)

# Data type check
print("NumPy dtype:", array.dtype)
print("Default torch.arange dtype:", torch.arange(1.0, 8.0).dtype)

# Modify the original array
array = array + 1
print("Modified NumPy array:", array)
print("Tensor after array change (no link anymore):", tensor)  # original tensor not affected

# PyTorch tensor to NumPy array
tensor = torch.ones(7)
numpy_tensor = tensor.numpy()

print("Original tensor:", tensor)
print("Converted NumPy array:", numpy_tensor)
print("NumPy dtype:", numpy_tensor.dtype)

# Modify tensor
tensor = tensor + 1
print("Modified tensor:", tensor)
print("NumPy array after tensor change (not affected):", numpy_tensor)

"""**Reproducibility in PyTorch using `torch.manual_seed()`**

* Fixing the seed ensures reproducible randomness.

"""

# Create two random tensors (without seed)
random_tensor_A = torch.rand(3, 4)
random_tensor_B = torch.rand(3, 4)

print("Random Tensor A:\n", random_tensor_A)
print("Random Tensor B:\n", random_tensor_B)
print("Are A and B equal?", random_tensor_A == random_tensor_B)  # likely False

# Reproducible random tensors
RANDOM_SEED = 42
torch.manual_seed(RANDOM_SEED)
random_tensor_C = torch.rand(3, 4)

torch.manual_seed(RANDOM_SEED)
random_tensor_D = torch.rand(3, 4)

print("Reproducible Tensor C:\n", random_tensor_C)
print("Reproducible Tensor D:\n", random_tensor_D)
print("Are C and D equal?", random_tensor_C == random_tensor_D)  # should be True

"""**GPU Support – CUDA / MPS / CPU device checking and usage**

* PyTorch allows automatic detection and device placement for efficient training.

"""

# Check for GPU availability
print("CUDA available?", torch.cuda.is_available())
print("CUDA device count:", torch.cuda.device_count())

# Check for Apple Silicon GPU (MPS)
print("MPS available? (for Mac)", torch.backends.mps.is_available())

# Set device agnostically
if torch.cuda.is_available():
    device = "cuda"
elif torch.backends.mps.is_available():
    device = "mps"
else:
    device = "cpu"

print("Device being used:", device)

# Create a tensor on CPU
tensor = torch.tensor([1, 2, 3])
print("Default tensor:", tensor)
print("Device:", tensor.device)

# Move tensor to GPU if available
tensor_on_gpu = tensor.to(device)
print("Tensor on device:", tensor_on_gpu)

# Try converting tensor on GPU directly to NumPy (will error!)
try:
    print(tensor_on_gpu.numpy())
except Exception as e:
    print("Error when converting GPU tensor to NumPy:", e)

# Correct way: move back to CPU first
tensor_back_on_cpu = tensor_on_gpu.cpu().numpy()
print("Tensor moved back to CPU and converted to NumPy:", tensor_back_on_cpu)